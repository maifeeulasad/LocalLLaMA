[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Sebastian Raschka is at it again! This time he compares the Qwen 3 and gpt-oss architectures. I'm looking forward to his deep dive, his Qwen 3 series was phenomenal.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Qwen3 vs. gpt-oss architecture: width matters",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 74,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mj00g7",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.94,
            "author_flair_background_color": "transparent",
            "ups": 256,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
            "is_original_content": false,
            "author_fullname": "t2_1a48h7vf",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 256,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/wkWrh1GN4jmRi4E3b7fiDo0FPy9CvieyioaUixss82k.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [
              {
                "a": ":X:",
                "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                "e": "emoji"
              }
            ],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754472471,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sebastian Raschka is at it again! This time he compares the Qwen 3 and gpt-oss architectures. I&amp;#39;m looking forward to his deep dive, his Qwen 3 series was phenomenal.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/vqgb87dfbdhf1.jpeg",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/vqgb87dfbdhf1.jpeg?auto=webp&amp;s=053debd943ce34e7a4882b98c600b03ceb5cf38f",
                    "width": 1477,
                    "height": 781
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/vqgb87dfbdhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae04d2f64f4bcd5577008902946ffde3b411133c",
                      "width": 108,
                      "height": 57
                    },
                    {
                      "url": "https://preview.redd.it/vqgb87dfbdhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9f2e46f2b2e0162601d78daaac0b737c7c7e6df4",
                      "width": 216,
                      "height": 114
                    },
                    {
                      "url": "https://preview.redd.it/vqgb87dfbdhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67ed17317f54c7ff360ecfb670f99a130af72db0",
                      "width": 320,
                      "height": 169
                    },
                    {
                      "url": "https://preview.redd.it/vqgb87dfbdhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6c2ff32aeda0494c869aa38d27852485afc947c7",
                      "width": 640,
                      "height": 338
                    },
                    {
                      "url": "https://preview.redd.it/vqgb87dfbdhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=efa5520fbbfb6bdfeb02d4cbbf3b20888973dc7e",
                      "width": 960,
                      "height": 507
                    },
                    {
                      "url": "https://preview.redd.it/vqgb87dfbdhf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=17d8c9e3b585fd6a8d5030ae18e5d05fa33a76f7",
                      "width": 1080,
                      "height": 571
                    }
                  ],
                  "variants": {},
                  "id": "OEAEco8LSIl6H_GMf5gyI3qUmqHV5UcH7ZWtbdBxeMk"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": ":X:",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mj00g7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "entsnack",
            "discussion_type": null,
            "num_comments": 46,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": "dark",
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/",
            "stickied": false,
            "url": "https://i.redd.it/vqgb87dfbdhf1.jpeg",
            "subreddit_subscribers": 512875,
            "created_utc": 1754472471,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n78o7zm",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "jakegh",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n785df0",
                                          "score": 8,
                                          "author_fullname": "t2_6vt1n",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I really don't think cannibalization is why GPT-OSS sucks so bad. My feeling is the problem really is their strict RL guiderails. The refusals are the problem. I got a refusal on SQL analytics for crying out loud!\n\nLooking forward to much smarter people than me investigating.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n78o7zm",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I really don&amp;#39;t think cannibalization is why GPT-OSS sucks so bad. My feeling is the problem really is their strict RL guiderails. The refusals are the problem. I got a refusal on SQL analytics for crying out loud!&lt;/p&gt;\n\n&lt;p&gt;Looking forward to much smarter people than me investigating.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj00g7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78o7zm/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754491640,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754491640,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 8
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n78rna7",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "huffalump1",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n785df0",
                                          "score": 4,
                                          "author_fullname": "t2_9bj7v",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Yup, feels like they did it just to avoid headlines of \"new model from ChatGPT maker writes kiddie porn and hates white/brown/female/male people\" etc.\n\nTheir own model safety report says that most of the safety measures can be fine-tuned away but it's not MORE dangerous than other open models, so, fuck it",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n78rna7",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yup, feels like they did it just to avoid headlines of &amp;quot;new model from ChatGPT maker writes kiddie porn and hates white/brown/female/male people&amp;quot; etc.&lt;/p&gt;\n\n&lt;p&gt;Their own model safety report says that most of the safety measures can be fine-tuned away but it&amp;#39;s not MORE dangerous than other open models, so, fuck it&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj00g7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78rna7/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754492614,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754492614,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 4
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n785df0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "FullstackSensei",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n77z0fo",
                                "score": 24,
                                "author_fullname": "t2_17n3nqtj56",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It's not that openAI engineers don't know any better. It's what happens when marketing and management want to make something for PR purposes but fear of competing with one's own paid models.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n785df0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not that openAI engineers don&amp;#39;t know any better. It&amp;#39;s what happens when marketing and management want to make something for PR purposes but fear of competing with one&amp;#39;s own paid models.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj00g7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n785df0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754485876,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754485876,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 24
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n78tcn8",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Affectionate-Cap-600",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n77z0fo",
                                "score": 3,
                                "author_fullname": "t2_5oltmr5b",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "&gt;That means the Embedding and Output matrix is a full 1.16b of both the 120b and the 20b model! It’s like 5% of the damn model.\n\n\nyeah and like 25% of the active parameters lmao\nqwen MoEs use tie embeddings = True they have only one matrix here",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n78tcn8",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;That means the Embedding and Output matrix is a full 1.16b of both the 120b and the 20b model! It’s like 5% of the damn model.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;yeah and like 25% of the active parameters lmao\nqwen MoEs use tie embeddings = True they have only one matrix here&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj00g7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78tcn8/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754493100,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754493100,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n77z0fo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "DistanceSolar1449",
                      "can_mod_post": false,
                      "created_utc": 1754483707,
                      "send_replies": true,
                      "parent_id": "t1_n77o2eb",
                      "score": 46,
                      "author_fullname": "t2_1utnp17o3h",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "You’re joking but the truth isn’t far off in that the massive vocab size is useless\n\nOpenAI copied that from the 120b model to the 20b model. That means the Embedding and Output matrix is a full 1.16b of both the 120b and the 20b model! It’s like 5% of the damn model.\n\nIn fact, openAI lied about the model being A3b, it’s actually A4.19B if you count both fat ass embedding matrices! OpenAI only counts one of them for some reason. ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n77z0fo",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You’re joking but the truth isn’t far off in that the massive vocab size is useless&lt;/p&gt;\n\n&lt;p&gt;OpenAI copied that from the 120b model to the 20b model. That means the Embedding and Output matrix is a full 1.16b of both the 120b and the 20b model! It’s like 5% of the damn model.&lt;/p&gt;\n\n&lt;p&gt;In fact, openAI lied about the model being A3b, it’s actually A4.19B if you count both fat ass embedding matrices! OpenAI only counts one of them for some reason. &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n77z0fo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754483707,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 46
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7deo10",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "LimpFeedback463",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7834ys",
                                "score": 1,
                                "author_fullname": "t2_t7k8pwzs",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "i heard someone saying that these open source models from OpenAI are purely trained on curated / synthetic data, so can that not be the case that they are meant to perform better at already present benchmarks??",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7deo10",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i heard someone saying that these open source models from OpenAI are purely trained on curated / synthetic data, so can that not be the case that they are meant to perform better at already present benchmarks??&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj00g7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7deo10/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754547658,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754547658,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7834ys",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "sumrix",
                      "can_mod_post": false,
                      "created_utc": 1754485125,
                      "send_replies": true,
                      "parent_id": "t1_n77o2eb",
                      "score": 8,
                      "author_fullname": "t2_3ae2vqzj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "In my tests, GPT-OSS 20B demonstrates better proficiency in the Tatar language than the Qwen3 30B and 32B models. So, I suppose that's one of its strengths.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7834ys",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In my tests, GPT-OSS 20B demonstrates better proficiency in the Tatar language than the Qwen3 30B and 32B models. So, I suppose that&amp;#39;s one of its strengths.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7834ys/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754485125,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 8
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n77o2eb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Cool-Chemical-5629",
            "can_mod_post": false,
            "created_utc": 1754479447,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 161,
            "author_fullname": "t2_qz1qjc86",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "GPT-OSS 20B vocabulary size of 200k\n\nQwen3 30B-A3B vocabulary size of 151k\n\nThat's extra 49k variants of \"Sorry, I can't provide that\"!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n77o2eb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GPT-OSS 20B vocabulary size of 200k&lt;/p&gt;\n\n&lt;p&gt;Qwen3 30B-A3B vocabulary size of 151k&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s extra 49k variants of &amp;quot;Sorry, I can&amp;#39;t provide that&amp;quot;!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n77o2eb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754479447,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 161
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n79725e",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": false,
                                                    "author": "FullstackSensei",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n790lsh",
                                                    "score": 3,
                                                    "author_fullname": "t2_17n3nqtj56",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "That representation is what each layer creates from the tokens it attended to.\n\nModels evolve at a much slower pace in AI labs than in academia. Each new paper takes the researchers and engineers out of their comfort zones, because it's not yet tried and tested at scale. They need to evaluate so many other things:\n* the architecture changes (ex: dense vs MoE, and the numerous architecture variations within each),\n* test each architecture with reasonably sized models that are large enough to show complex behavior but small enough so as not to be too expensive and time consuming to train, \n* decide on concrete numbers for all the details of the architecture to hit a certain number of model parameters,\n* evolve their training data from the learnings of the last model they trained and shipped,\n* and they still need to train and ship a competitive new model, all within the span of 6-8 months.\n\nThere's only so much you can change given the time pressure before risks become too high to manage. Look at what happened with Llama 4. Even the original Qwen 3 MoE releases were considered underwhelming by a lot of people when compared to 2.5.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n79725e",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That representation is what each layer creates from the tokens it attended to.&lt;/p&gt;\n\n&lt;p&gt;Models evolve at a much slower pace in AI labs than in academia. Each new paper takes the researchers and engineers out of their comfort zones, because it&amp;#39;s not yet tried and tested at scale. They need to evaluate so many other things:\n* the architecture changes (ex: dense vs MoE, and the numerous architecture variations within each),\n* test each architecture with reasonably sized models that are large enough to show complex behavior but small enough so as not to be too expensive and time consuming to train, \n* decide on concrete numbers for all the details of the architecture to hit a certain number of model parameters,\n* evolve their training data from the learnings of the last model they trained and shipped,\n* and they still need to train and ship a competitive new model, all within the span of 6-8 months.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s only so much you can change given the time pressure before risks become too high to manage. Look at what happened with Llama 4. Even the original Qwen 3 MoE releases were considered underwhelming by a lot of people when compared to 2.5.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mj00g7",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n79725e/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754496948,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754496948,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 3
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n790lsh",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "Affectionate-Cap-600",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n78vrm0",
                                          "score": 1,
                                          "author_fullname": "t2_5oltmr5b",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "yeah that's true, to be honest that's probably not the reason that limit its performance\n\n&gt;way more than a human can hold \"in flight\" when reading a new text.\n\nyeah but (if we look at that in this way) we create a representation for the 'past tokens'. we don't have to go back word to word because we compress the concept.\nin this way (in my view obviously) how we look is more like linear attention (as we compare words to an aggregate representations of past words), or even a LSTM in some aspects. \n\nconceptually, I always considered inteleaving linear / softmax attention to be more 'appealing' than using a sliding window. \nyeah, you have to solve the cumsum problem (for causal language modelling, that's not needed for encoder only models) but it is possible, just look at lightning attention from minimax ('in their paper they evaluate iSWA, from 64 to 1024 tokens of local context, but they found that linear attention outperform any sliding window, when interleaved)",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n790lsh",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah that&amp;#39;s true, to be honest that&amp;#39;s probably not the reason that limit its performance&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;way more than a human can hold &amp;quot;in flight&amp;quot; when reading a new text.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;yeah but (if we look at that in this way) we create a representation for the &amp;#39;past tokens&amp;#39;. we don&amp;#39;t have to go back word to word because we compress the concept.\nin this way (in my view obviously) how we look is more like linear attention (as we compare words to an aggregate representations of past words), or even a LSTM in some aspects. &lt;/p&gt;\n\n&lt;p&gt;conceptually, I always considered inteleaving linear / softmax attention to be more &amp;#39;appealing&amp;#39; than using a sliding window. \nyeah, you have to solve the cumsum problem (for causal language modelling, that&amp;#39;s not needed for encoder only models) but it is possible, just look at lightning attention from minimax (&amp;#39;in their paper they evaluate iSWA, from 64 to 1024 tokens of local context, but they found that linear attention outperform any sliding window, when interleaved)&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj00g7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n790lsh/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754495142,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754495142,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n78vrm0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "FullstackSensei",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n78tql1",
                                "score": 1,
                                "author_fullname": "t2_17n3nqtj56",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Most recent models use the same or similar attention mechanisms to convert attention from quadratic to linear, but don't suffer from any limitations.\n\nThink about it this way, 128 tokens is way more than a human can hold \"in flight\" when reading a new text. Even if they used the sliding window on all 24 layers of the 20B model, that's a maximum of 3k different tokens that can be attended to across all layers, and that's to predict one output token only. The next token can attend to a different set of tokens.\n\nI really don't think this poses any limitation on the model's ability to attend to the proper tokens within the context. Rather, the lack of depth prevents it from learning enough abstractions to be able to grasp complex enough concepts. Couple that with the neutering it got from safety training, and you got yourself a perfect recipe for mediocrity.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n78vrm0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Most recent models use the same or similar attention mechanisms to convert attention from quadratic to linear, but don&amp;#39;t suffer from any limitations.&lt;/p&gt;\n\n&lt;p&gt;Think about it this way, 128 tokens is way more than a human can hold &amp;quot;in flight&amp;quot; when reading a new text. Even if they used the sliding window on all 24 layers of the 20B model, that&amp;#39;s a maximum of 3k different tokens that can be attended to across all layers, and that&amp;#39;s to predict one output token only. The next token can attend to a different set of tokens.&lt;/p&gt;\n\n&lt;p&gt;I really don&amp;#39;t think this poses any limitation on the model&amp;#39;s ability to attend to the proper tokens within the context. Rather, the lack of depth prevents it from learning enough abstractions to be able to grasp complex enough concepts. Couple that with the neutering it got from safety training, and you got yourself a perfect recipe for mediocrity.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj00g7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78vrm0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754493783,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754493783,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n78uo1f",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "dinerburgeryum",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n78tql1",
                                "score": 0,
                                "author_fullname": "t2_1j53p3yv3e",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "That's one way to consider iSWA, but also: it allows more focus on local information and cuts down memory requirements substantially. Especially with GQA you can really get lost in the weeds with full attention on every layer.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n78uo1f",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s one way to consider iSWA, but also: it allows more focus on local information and cuts down memory requirements substantially. Especially with GQA you can really get lost in the weeds with full attention on every layer.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj00g7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78uo1f/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754493473,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754493473,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 0
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n78tql1",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Affectionate-Cap-600",
                      "can_mod_post": false,
                      "created_utc": 1754493209,
                      "send_replies": true,
                      "parent_id": "t1_n783wqw",
                      "score": 5,
                      "author_fullname": "t2_5oltmr5b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;more depth \"enriches\" the context each layer has when selecting which tokens to attend to.\n\nwell... also this model has a sliding window of 128 tokens on half of the layers, so that limit the expressiveness of attention a lot",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78tql1",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;more depth &amp;quot;enriches&amp;quot; the context each layer has when selecting which tokens to attend to.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;well... also this model has a sliding window of 128 tokens on half of the layers, so that limit the expressiveness of attention a lot&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78tql1/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754493209,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7d2ron",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "Howard_banister",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7bhpug",
                                          "score": 1,
                                          "author_fullname": "t2_27jg5ljp",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "That’s an interesting architectural choice—they did the exact opposite of what Kimi K2 did.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7d2ron",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s an interesting architectural choice—they did the exact opposite of what Kimi K2 did.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj00g7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7d2ron/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754541664,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754541664,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7bhpug",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "notdba",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7a8x3a",
                                "score": 2,
                                "author_fullname": "t2_4aai0pnm",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "GLM-4.5 did the opposite:\n\n\\&gt; Unlike DeepSeek-V3 and Kimi K2, we reduce the width (hidden dimension and number of routed experts) of the model while increasing the height (number of layers), as we found that deeper models exhibit better reasoning capacity.\n\n([https://z.ai/blog/glm-4.5](https://z.ai/blog/glm-4.5))\n\nSo far I like GLM-4.5 (355B-A32B) more than Qwen3-Coder (480B-A35B).",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7bhpug",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GLM-4.5 did the opposite:&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Unlike DeepSeek-V3 and Kimi K2, we reduce the width (hidden dimension and number of routed experts) of the model while increasing the height (number of layers), as we found that deeper models exhibit better reasoning capacity.&lt;/p&gt;\n\n&lt;p&gt;(&lt;a href=\"https://z.ai/blog/glm-4.5\"&gt;https://z.ai/blog/glm-4.5&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;So far I like GLM-4.5 (355B-A32B) more than Qwen3-Coder (480B-A35B).&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj00g7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7bhpug/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754520847,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754520847,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7a8x3a",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Howard_banister",
                      "can_mod_post": false,
                      "created_utc": 1754507529,
                      "send_replies": true,
                      "parent_id": "t1_n783wqw",
                      "score": 3,
                      "author_fullname": "t2_27jg5ljp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Depth enables richer abstractions, yet it re-amplifies the vanishing-gradient problem. Residual blocks, LayerNorm, and attention only slow the exponential decay of ∂L/∂x with depth; they do not eliminate it. SmolLM works because it stays shallow (≤ 30 layers).\n\nIf I’m not mistaken, many open-source models now favor a shallow-and-wide design such as DeepSeek-V3, Kimi k2 as well as new Qwen3-Coder, despite being larger than its 235 B predecessor (480 B vs 235 B), it actually reduced depth (63 vs 94 layers).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7a8x3a",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Depth enables richer abstractions, yet it re-amplifies the vanishing-gradient problem. Residual blocks, LayerNorm, and attention only slow the exponential decay of ∂L/∂x with depth; they do not eliminate it. SmolLM works because it stays shallow (≤ 30 layers).&lt;/p&gt;\n\n&lt;p&gt;If I’m not mistaken, many open-source models now favor a shallow-and-wide design such as DeepSeek-V3, Kimi k2 as well as new Qwen3-Coder, despite being larger than its 235 B predecessor (480 B vs 235 B), it actually reduced depth (63 vs 94 layers).&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7a8x3a/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754507529,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n78q8wl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "entsnack",
                      "can_mod_post": false,
                      "created_utc": 1754492213,
                      "send_replies": true,
                      "parent_id": "t1_n783wqw",
                      "score": 2,
                      "author_fullname": "t2_1a48h7vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "interesting, will check out SmolLM",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78q8wl",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "a": ":X:",
                          "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                          "e": "emoji"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;interesting, will check out SmolLM&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78q8wl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754492213,
                      "author_flair_text": ":X:",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "transparent",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n78xivm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": "LOW_SCORE",
                      "no_follow": false,
                      "author": "orrzxz",
                      "can_mod_post": false,
                      "created_utc": 1754494284,
                      "send_replies": true,
                      "parent_id": "t1_n783wqw",
                      "score": -5,
                      "author_fullname": "t2_huzar",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It's a well established fact in any proffesional field.\n\nI am really not sure why it took years for people in the ML field to catch onto the gist that smaller, more specialized == better\n\n'Jack of all trades, master of none\" has been a saying since... forever, basically.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78xivm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a well established fact in any proffesional field.&lt;/p&gt;\n\n&lt;p&gt;I am really not sure why it took years for people in the ML field to catch onto the gist that smaller, more specialized == better&lt;/p&gt;\n\n&lt;p&gt;&amp;#39;Jack of all trades, master of none&amp;quot; has been a saying since... forever, basically.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": "comment score below threshold",
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78xivm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754494284,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": true,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -5
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n783wqw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1754485387,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 33,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "IIRC, it's a well established \"fact\" in the ML community that depth trumps width, even more so since the dawn of attention. Depth enables a model to \"work with\" higher level abstractions. Since all attention blocks across all layers have access to all the input, more depth \"enriches\" the context each layer has when selecting which tokens to attend to. The SmolLM family from HF are a prime demonstration of this.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n783wqw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;IIRC, it&amp;#39;s a well established &amp;quot;fact&amp;quot; in the ML community that depth trumps width, even more so since the dawn of attention. Depth enables a model to &amp;quot;work with&amp;quot; higher level abstractions. Since all attention blocks across all layers have access to all the input, more depth &amp;quot;enriches&amp;quot; the context each layer has when selecting which tokens to attend to. The SmolLM family from HF are a prime demonstration of this.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n783wqw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754485387,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 33
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n792kn2",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "olddoglearnsnewtrick",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n78v0jd",
                                          "score": 3,
                                          "author_fullname": "t2_tbasi",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "TYVM",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n792kn2",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;TYVM&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj00g7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n792kn2/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754495686,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754495686,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n79n13n",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "bucolucas",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n78v0jd",
                                          "score": 3,
                                          "author_fullname": "t2_ai9fq",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "oh man that's an awesome article",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n79n13n",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "Llama 3.1"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;oh man that&amp;#39;s an awesome article&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj00g7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n79n13n/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754501382,
                                          "author_flair_text": "Llama 3.1",
                                          "treatment_tags": [],
                                          "created_utc": 1754501382,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#93b1ba",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n78v0jd",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "dinerburgeryum",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n78ti3k",
                                "score": 5,
                                "author_fullname": "t2_1j53p3yv3e",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "If you're interested in the attention sink concept, check out [Attention Is Off By One](https://www.evanmiller.org/attention-is-off-by-one.html). It's remarkably accessible for a post about math, and has a fun cheeky tone to it as well.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n78v0jd",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re interested in the attention sink concept, check out &lt;a href=\"https://www.evanmiller.org/attention-is-off-by-one.html\"&gt;Attention Is Off By One&lt;/a&gt;. It&amp;#39;s remarkably accessible for a post about math, and has a fun cheeky tone to it as well.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj00g7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78v0jd/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754493572,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754493572,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 5
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n78ti3k",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "olddoglearnsnewtrick",
                      "can_mod_post": false,
                      "created_utc": 1754493142,
                      "send_replies": true,
                      "parent_id": "t1_n78l3dl",
                      "score": 6,
                      "author_fullname": "t2_tbasi",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "When I grow up I want to understand things like you do Sir.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78ti3k",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When I grow up I want to understand things like you do Sir.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78ti3k/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754493142,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n78pe91",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "entsnack",
                      "can_mod_post": false,
                      "created_utc": 1754491972,
                      "send_replies": true,
                      "parent_id": "t1_n78l3dl",
                      "score": 5,
                      "author_fullname": "t2_1a48h7vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah I noticed the absence of attention sinks too, Raschka talks about them but they're not in his diagram.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78pe91",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "a": ":X:",
                          "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                          "e": "emoji"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah I noticed the absence of attention sinks too, Raschka talks about them but they&amp;#39;re not in his diagram.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78pe91/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754491972,
                      "author_flair_text": ":X:",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "transparent",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7e0z50",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "sciencewarrior",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7cyacu",
                                          "score": 1,
                                          "author_fullname": "t2_4feaa",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Got it, thank you!",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7e0z50",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Got it, thank you!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj00g7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7e0z50/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754560334,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754560334,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7cyacu",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "dinerburgeryum",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7co9u7",
                                "score": 1,
                                "author_fullname": "t2_1j53p3yv3e",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Couple things:\n\nKV is absolutely compressible in a general sense. llama.cpp bottoms out at 8-bit quant in practical terms. Exllamav2 and beyond do a better job of eating outliers so you get excellent results at 4 bits over there. \n\nIn this case, however, I’m talking about sliding window attention, which is a fancy way of saying that every other layer* only attends to a small, recent slice of the overall context. Rather than every layer looking at potentially 64k tokens or something, now half your layers are only looking at the most recent 128 tokens. This means your KV cache is about half the size it would otherwise be. \n\n* this is called interleaved sliding attention and I’m using it as an example here. ",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7cyacu",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Couple things:&lt;/p&gt;\n\n&lt;p&gt;KV is absolutely compressible in a general sense. llama.cpp bottoms out at 8-bit quant in practical terms. Exllamav2 and beyond do a better job of eating outliers so you get excellent results at 4 bits over there. &lt;/p&gt;\n\n&lt;p&gt;In this case, however, I’m talking about sliding window attention, which is a fancy way of saying that every other layer* only attends to a small, recent slice of the overall context. Rather than every layer looking at potentially 64k tokens or something, now half your layers are only looking at the most recent 128 tokens. This means your KV cache is about half the size it would otherwise be. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;this is called interleaved sliding attention and I’m using it as an example here. &lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj00g7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7cyacu/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754539661,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754539661,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7co9u7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "sciencewarrior",
                      "can_mod_post": false,
                      "created_utc": 1754535578,
                      "send_replies": true,
                      "parent_id": "t1_n78l3dl",
                      "score": 1,
                      "author_fullname": "t2_4feaa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I was under the impression that KV cache wasn't compressible, so 128k at fp16 would take about 20GB. Am I missing something important here?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7co9u7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was under the impression that KV cache wasn&amp;#39;t compressible, so 128k at fp16 would take about 20GB. Am I missing something important here?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7co9u7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754535578,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n78l3dl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "dinerburgeryum",
            "can_mod_post": false,
            "created_utc": 1754490743,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 19,
            "author_fullname": "t2_1j53p3yv3e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I said this on the other post, but this diagram misses the attention sinks, the importance of which can't be overstated when you're talking about quantized models. Qwen also does not use interleaved SWA, which GPT-OSS does; this reduces the KV cache size requirements by a non-trivial amount, especially when you're talking about edge deployment. This diagram is misleading at best.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n78l3dl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I said this on the other post, but this diagram misses the attention sinks, the importance of which can&amp;#39;t be overstated when you&amp;#39;re talking about quantized models. Qwen also does not use interleaved SWA, which GPT-OSS does; this reduces the KV cache size requirements by a non-trivial amount, especially when you&amp;#39;re talking about edge deployment. This diagram is misleading at best.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78l3dl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754490743,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 19
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n78vd1w",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "entsnack",
                      "can_mod_post": false,
                      "created_utc": 1754493668,
                      "send_replies": true,
                      "parent_id": "t1_n78twqm",
                      "score": 2,
                      "author_fullname": "t2_1a48h7vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "He usually does a deep dive into the architectures but it's not out yet.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78vd1w",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "a": ":X:",
                          "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                          "e": "emoji"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;He usually does a deep dive into the architectures but it&amp;#39;s not out yet.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78vd1w/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754493668,
                      "author_flair_text": ":X:",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "transparent",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n78twqm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Affectionate-Cap-600",
            "can_mod_post": false,
            "created_utc": 1754493256,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 8,
            "author_fullname": "t2_5oltmr5b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "this image don't mention that half of the layers of OSS use a sliding window of 128 tokens...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n78twqm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;this image don&amp;#39;t mention that half of the layers of OSS use a sliding window of 128 tokens...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78twqm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754493256,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n78y3uk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Accomplished-Copy332",
                      "can_mod_post": false,
                      "created_utc": 1754494450,
                      "send_replies": true,
                      "parent_id": "t1_n77wqso",
                      "score": 2,
                      "author_fullname": "t2_98ouo03z",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yet oss isn't really on the level of Qwen3 at all.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78y3uk",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yet oss isn&amp;#39;t really on the level of Qwen3 at all.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78y3uk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754494450,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n77wqso",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Parking_Outcome4557",
            "can_mod_post": false,
            "created_utc": 1754482883,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 8,
            "author_fullname": "t2_1ukzrwxpex",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "do you think they just copied architecture of qwen3 or this just common architecture?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n77wqso",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;do you think they just copied architecture of qwen3 or this just common architecture?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n77wqso/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754482883,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7aqejl",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "entsnack",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7apoti",
                                          "score": 2,
                                          "author_fullname": "t2_1a48h7vf",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "It improves inference speed but it may come with the tradeoff of performance on important benchmarks. So \"better\" is poorly defined.\n\nWhy I posted this is because some of us are academically interested in understanding architecture choices and how they interact with different engineering constraints. Engineering is all about tradeoffs, so if you can trace back an architectural change to a tradeoff, you can use it to design new architectures or apply old architectures for new tasks.\n\nSorry for rambling, but Sebastian Raschka says is it much better. Check out his Qwen architecture series, absolute gold content.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7aqejl",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "a": ":X:",
                                              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                                              "e": "emoji"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It improves inference speed but it may come with the tradeoff of performance on important benchmarks. So &amp;quot;better&amp;quot; is poorly defined.&lt;/p&gt;\n\n&lt;p&gt;Why I posted this is because some of us are academically interested in understanding architecture choices and how they interact with different engineering constraints. Engineering is all about tradeoffs, so if you can trace back an architectural change to a tradeoff, you can use it to design new architectures or apply old architectures for new tasks.&lt;/p&gt;\n\n&lt;p&gt;Sorry for rambling, but Sebastian Raschka says is it much better. Check out his Qwen architecture series, absolute gold content.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj00g7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "dark",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7aqejl/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754512495,
                                          "author_flair_text": ":X:",
                                          "treatment_tags": [],
                                          "created_utc": 1754512495,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "transparent",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7apoti",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "MrPrivateObservation",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7aldos",
                                "score": 1,
                                "author_fullname": "t2_42wlymjd",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "why else should width matter when it doesn't matter to be better?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7apoti",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;why else should width matter when it doesn&amp;#39;t matter to be better?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj00g7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7apoti/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754512290,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754512290,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7aldos",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "entsnack",
                      "can_mod_post": false,
                      "created_utc": 1754511087,
                      "send_replies": true,
                      "parent_id": "t1_n7aktn8",
                      "score": 2,
                      "author_fullname": "t2_1a48h7vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Who said one architecture is better than the other?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7aldos",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "a": ":X:",
                          "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                          "e": "emoji"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Who said one architecture is better than the other?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7aldos/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754511087,
                      "author_flair_text": ":X:",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "transparent",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7aktn8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "MrPrivateObservation",
            "can_mod_post": false,
            "created_utc": 1754510932,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 2,
            "author_fullname": "t2_42wlymjd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Were they trained on the same data? If not than they are not comparable as we don't know which model design is actually better.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7aktn8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Were they trained on the same data? If not than they are not comparable as we don&amp;#39;t know which model design is actually better.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7aktn8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754510932,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7ceuxp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "SomeAcanthocephala17",
                      "can_mod_post": false,
                      "created_utc": 1754532115,
                      "send_replies": true,
                      "parent_id": "t1_n77nvg5",
                      "score": 1,
                      "author_fullname": "t2_8xw6jkvs",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "actually a new one came out 4 days ago (before this gpt release) it's also A3B but has a number behind it, I think 2501 (and know that qwen a3B is actualy 3,3b), so I wonder how this new qwen3 update model compares to the 20B model of gpt-oss",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7ceuxp",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;actually a new one came out 4 days ago (before this gpt release) it&amp;#39;s also A3B but has a number behind it, I think 2501 (and know that qwen a3B is actualy 3,3b), so I wonder how this new qwen3 update model compares to the 20B model of gpt-oss&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7ceuxp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754532115,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n77nvg5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ArchdukeofHyperbole",
            "can_mod_post": false,
            "created_utc": 1754479365,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 2,
            "author_fullname": "t2_1p41v97q5d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have a feeling the next qwen would have settings more similar to oss, but with better performance.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n77nvg5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a feeling the next qwen would have settings more similar to oss, but with better performance.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n77nvg5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754479365,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n78pums",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "entsnack",
                      "can_mod_post": false,
                      "created_utc": 1754492101,
                      "send_replies": true,
                      "parent_id": "t1_n782yek",
                      "score": 2,
                      "author_fullname": "t2_1a48h7vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Sure! https://www.reddit.com/r/LocalLLaMA/comments/1lgy4wa/build_qwen3_from_scratch/",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78pums",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "a": ":X:",
                          "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                          "e": "emoji"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sure! &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lgy4wa/build_qwen3_from_scratch/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lgy4wa/build_qwen3_from_scratch/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n78pums/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754492101,
                      "author_flair_text": ":X:",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "transparent",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7cf7h5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "SomeAcanthocephala17",
                      "can_mod_post": false,
                      "created_utc": 1754532238,
                      "send_replies": true,
                      "parent_id": "t1_n782yek",
                      "score": 1,
                      "author_fullname": "t2_8xw6jkvs",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That link below is old, QWEN3 released new updated models a few days ago",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7cf7h5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That link below is old, QWEN3 released new updated models a few days ago&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj00g7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7cf7h5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754532238,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n782yek",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Luca3700",
            "can_mod_post": false,
            "created_utc": 1754485063,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 1,
            "author_fullname": "t2_cmkzm6vc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Can you provide the link for the qwen 3 series? Thank you",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n782yek",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can you provide the link for the qwen 3 series? Thank you&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n782yek/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754485063,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7a0q91",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "vertigo235",
            "can_mod_post": false,
            "created_utc": 1754505202,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 1,
            "author_fullname": "t2_812in",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\\#TWSS",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7a0q91",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;#TWSS&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7a0q91/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754505202,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ceoku",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "SomeAcanthocephala17",
            "can_mod_post": false,
            "created_utc": 1754532053,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 1,
            "author_fullname": "t2_8xw6jkvs",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Did you use the latest qwen3 A3B 2501 model that was released last week to compare?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ceoku",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Did you use the latest qwen3 A3B 2501 model that was released last week to compare?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7ceoku/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754532053,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dtjqm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "custodiam99",
            "can_mod_post": false,
            "created_utc": 1754556085,
            "send_replies": true,
            "parent_id": "t3_1mj00g7",
            "score": 1,
            "author_fullname": "t2_nqnhgqqf5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The two models have quite different vibes, but I like gpt-oss too. Also it is not impossible that they will release new (last year's SOTA) models every year (this way it won't disturb their paid operations).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dtjqm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The two models have quite different vibes, but I like gpt-oss too. Also it is not impossible that they will release new (last year&amp;#39;s SOTA) models every year (this way it won&amp;#39;t disturb their paid operations).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/n7dtjqm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754556085,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj00g7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]