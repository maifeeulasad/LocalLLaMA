[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I downloaded 2 8bit models (both use 32-33gb of ram)\n\nThe first one was Qwen3 30B A3B Instruct 2507 8bit. This model is much nicer it seems more \"Human like\" ie like a Nexus 6 vs a Nexus 4 etc.. The answers and modeled behaviors are much more interesting and personable. faster ie 72 tokens per second\n\nThe second one Qwen3 32B 8bit 8BIT seems more like just getting wikipedia answers, more of a formal Rigid feel to its behavior. slower ie less tokens per second 15 tokens per second.\n\nSo is the first one more advanced version? Why is it so different in how it behaves it defiantly is the one I will stick with.  Significantly nicer \"Attitude as well\" \n\nAnyhow this AI stuff is do damn interesting downloading more models to check out. I am using LM-Studio because it supports MLX.\n\nSo what's going on with these models?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "two models big difference in how it converses/answers. ie Qwen3 30B A3B vs Qwen3 32B",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mkgv1l",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_q4dwaqj",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754612966,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded 2 8bit models (both use 32-33gb of ram)&lt;/p&gt;\n\n&lt;p&gt;The first one was Qwen3 30B A3B Instruct 2507 8bit. This model is much nicer it seems more &amp;quot;Human like&amp;quot; ie like a Nexus 6 vs a Nexus 4 etc.. The answers and modeled behaviors are much more interesting and personable. faster ie 72 tokens per second&lt;/p&gt;\n\n&lt;p&gt;The second one Qwen3 32B 8bit 8BIT seems more like just getting wikipedia answers, more of a formal Rigid feel to its behavior. slower ie less tokens per second 15 tokens per second.&lt;/p&gt;\n\n&lt;p&gt;So is the first one more advanced version? Why is it so different in how it behaves it defiantly is the one I will stick with.  Significantly nicer &amp;quot;Attitude as well&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;Anyhow this AI stuff is do damn interesting downloading more models to check out. I am using LM-Studio because it supports MLX.&lt;/p&gt;\n\n&lt;p&gt;So what&amp;#39;s going on with these models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mkgv1l",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "meshreplacer",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mkgv1l/two_models_big_difference_in_how_it/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkgv1l/two_models_big_difference_in_how_it/",
            "subreddit_subscribers": 513813,
            "created_utc": 1754612966,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7it35l",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "meshreplacer",
                      "can_mod_post": false,
                      "created_utc": 1754615359,
                      "send_replies": true,
                      "parent_id": "t1_n7iobpz",
                      "score": 1,
                      "author_fullname": "t2_q4dwaqj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Interesting. now the MOE model offers to even draw ASCII diagrams of things I asked. I did ask it an esoteric question regarding background gamma radiation and heavy rainfall. not only did it answer but it went into a good in-depth description and even drew a chart via ascii and elaborated on more details.\n\nIt even corrected me when I accidentally used the wrong isotope as well. This new golden age of AI is pretty fascinating. I remember the last era when Lisp and Symbolic workstations were the hot things on the market then AI winter for decades.\n\n  \nNow I wish I bought a Mac with even more memory I hear there are even larger 128gb models you can run locally.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7it35l",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting. now the MOE model offers to even draw ASCII diagrams of things I asked. I did ask it an esoteric question regarding background gamma radiation and heavy rainfall. not only did it answer but it went into a good in-depth description and even drew a chart via ascii and elaborated on more details.&lt;/p&gt;\n\n&lt;p&gt;It even corrected me when I accidentally used the wrong isotope as well. This new golden age of AI is pretty fascinating. I remember the last era when Lisp and Symbolic workstations were the hot things on the market then AI winter for decades.&lt;/p&gt;\n\n&lt;p&gt;Now I wish I bought a Mac with even more memory I hear there are even larger 128gb models you can run locally.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkgv1l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkgv1l/two_models_big_difference_in_how_it/n7it35l/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754615359,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7iobpz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sleepingsysadmin",
            "can_mod_post": false,
            "created_utc": 1754613679,
            "send_replies": true,
            "parent_id": "t3_1mkgv1l",
            "score": 3,
            "author_fullname": "t2_x73h4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The 30B A3B model is MOE model. The A3B tells you that it's 3B active. So you get the speed of a model like it was 3B but the intelligence of a 30B model. \n\nQwen3 32B is dense, not moe, and so it's slower. \n\n  \nNow if we assume same training material, same everything. The MOE model will output lower quality and less reliable answers. \n\nBut consider that we're assuming everything the same. When they arent the same. A MOE model could easily outperform much bigger models of lower quality.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7iobpz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The 30B A3B model is MOE model. The A3B tells you that it&amp;#39;s 3B active. So you get the speed of a model like it was 3B but the intelligence of a 30B model. &lt;/p&gt;\n\n&lt;p&gt;Qwen3 32B is dense, not moe, and so it&amp;#39;s slower. &lt;/p&gt;\n\n&lt;p&gt;Now if we assume same training material, same everything. The MOE model will output lower quality and less reliable answers. &lt;/p&gt;\n\n&lt;p&gt;But consider that we&amp;#39;re assuming everything the same. When they arent the same. A MOE model could easily outperform much bigger models of lower quality.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkgv1l/two_models_big_difference_in_how_it/n7iobpz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754613679,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkgv1l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7iordr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Thedudely1",
            "can_mod_post": false,
            "created_utc": 1754613839,
            "send_replies": true,
            "parent_id": "t3_1mkgv1l",
            "score": 3,
            "author_fullname": "t2_i305y",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen 3 30B A3B 2507 just got released and one of the biggest improvements was made to tone it seems, but Qwen 3 32B hasn't been updated and is still a part of the original Qwen 3 release. I've found Qwen 3 32B actually has better tone than the original Qwen 3 30B A3B does.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7iordr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen 3 30B A3B 2507 just got released and one of the biggest improvements was made to tone it seems, but Qwen 3 32B hasn&amp;#39;t been updated and is still a part of the original Qwen 3 release. I&amp;#39;ve found Qwen 3 32B actually has better tone than the original Qwen 3 30B A3B does.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkgv1l/two_models_big_difference_in_how_it/n7iordr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754613839,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkgv1l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7iq1r0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Herr_Drosselmeyer",
            "can_mod_post": false,
            "created_utc": 1754614290,
            "send_replies": true,
            "parent_id": "t3_1mkgv1l",
            "score": 1,
            "author_fullname": "t2_1zr9gwsn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The tone of the answers depends as much on the prompt than it does on the model. You can tell it in the system prompt how it should act and both of those should be able to handle that just fine. Just tell them to be more conversational or something like that.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7iq1r0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The tone of the answers depends as much on the prompt than it does on the model. You can tell it in the system prompt how it should act and both of those should be able to handle that just fine. Just tell them to be more conversational or something like that.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkgv1l/two_models_big_difference_in_how_it/n7iq1r0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754614290,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkgv1l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]